{
  "hash": "483b607d2370bfd5717acbb3fa5b5ed2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Correlation and Regression\"\n---\n\n\n\n\n## Introduction\n\nCorrelation and linear regression are techniques for describing the relationships between variables.\n\n**Correlation** looks for a linear association between two variables. The strength of the association is summarised by the correlation coefficient.\n\n**Regression** looks for the dependence of one variable (the dependent variable) on another (the independent variable). It quantifies the best linear relation between the variables and allows the prediction of the dependent variable when only the independent variable is known.\n\n## Correlation\n\nCorrelation is used to measure the degree of **linear association** between two continuous variables.\n\n### Types of Correlation Coefficients\n\nThere are two main types of correlation coefficients:\n\n| Coefficient | When to Use |\n|-------------|-------------|\n| **Pearson's correlation coefficient** | Data are approximately Normally distributed |\n| **Spearman's rank correlation coefficient** | Non-parametric alternative - use if data are not approximately normally distributed, have extreme values (outliers), or the sample size is small |\n\n### Interpreting the Correlation Coefficient\n\nThe correlation coefficient (**r**) can take any value in the range **-1 to +1**.\n\n**The sign** of the correlation coefficient indicates:\n\n- **Positive r**: One variable increases as the other variable increases\n- **Negative r**: One variable decreases as the other increases\n\n**The magnitude** of the correlation coefficient indicates the strength of the linear association:\n\n- **r = +1 or −1**: Perfect correlation. If both variables were plotted on a scatter graph all the points would lie on a straight line\n- **r = 0**: No linear correlation\n- The closer r is to -1 or 1, the greater the degree of linear association\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Strong positive\nn <- 50\nstrong_pos <- tibble(\n  x = rnorm(n, 50, 10),\n  y = 0.9 * x + rnorm(n, 0, 3),\n  type = \"(a) Strong positive\\nr ≈ 0.95\"\n)\n\n# Weak positive\nweak_pos <- tibble(\n  x = rnorm(n, 50, 10),\n  y = 0.3 * x + rnorm(n, 35, 8),\n  type = \"(b) Weak positive\\nr ≈ 0.35\"\n)\n\n# Uncorrelated\nuncorr <- tibble(\n  x = rnorm(n, 50, 10),\n  y = rnorm(n, 50, 10),\n  type = \"(c) Uncorrelated\\nr ≈ 0\"\n)\n\n# Weak negative\nweak_neg <- tibble(\n  x = rnorm(n, 50, 10),\n  y = -0.4 * x + rnorm(n, 70, 8),\n  type = \"(d) Weak negative\\nr ≈ -0.40\"\n)\n\nall_corr <- bind_rows(strong_pos, weak_pos, uncorr, weak_neg)\n\nggplot(all_corr, aes(x = x, y = y)) +\n  geom_point(colour = \"#2c3e50\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(x = \"x\", y = \"y\") +\n  theme_minimal(base_size = 12)\n```\n\n::: {.cell-output-display}\n![Scatterplots showing datasets with different correlations](09-correlation-regression_files/figure-html/fig-correlation-examples-1.png){#fig-correlation-examples width=960}\n:::\n:::\n\n\n### Important Cautions\n\n::: {.callout-warning}\n## Correlation Does Not Imply Causation\nIt is important to remember that a correlation between two variables does not necessarily imply a 'cause and effect' relationship.\n:::\n\nCorrelation refers only to **linear relationships**. A correlation of 0 means there is no linear relationship between the two variables. However, relationships between variables may still exist but be **non-linear**. It is always important to look at plots of data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(456)\nn <- 50\n\n# Quadratic relationship\nquad <- tibble(\n  x = seq(-3, 3, length.out = n),\n  y = x^2 + rnorm(n, 0, 0.5),\n  type = \"Quadratic (r ≈ 0)\"\n)\n\n# Sinusoidal\nsine <- tibble(\n  x = seq(0, 2*pi, length.out = n),\n  y = sin(x) + rnorm(n, 0, 0.2),\n  type = \"Sinusoidal (r ≈ 0)\"\n)\n\n# Outlier effect\noutlier <- tibble(\n  x = c(rnorm(n-1, 50, 5), 100),\n  y = c(rnorm(n-1, 50, 5), 100),\n  type = \"Outlier influence\"\n)\n\nnonlinear <- bind_rows(quad, sine, outlier)\n\nggplot(nonlinear, aes(x = x, y = y)) +\n  geom_point(colour = \"#e74c3c\", alpha = 0.7) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(x = \"x\", y = \"y\") +\n  theme_minimal(base_size = 12)\n```\n\n::: {.cell-output-display}\n![Examples where the correlation coefficient is misleading - non-linear relationships](09-correlation-regression_files/figure-html/fig-nonlinear-1.png){#fig-nonlinear width=960}\n:::\n:::\n\n\n::: {.callout-warning}\n## When NOT to Use Correlation\n\nCorrelation analysis is **inappropriate** in these situations:\n\n1. **Non-linear relationships**: As shown above, quadratic or curved relationships will have r ≈ 0 even though a strong relationship exists.\n\n2. **Categorical data**: You cannot calculate a meaningful Pearson correlation between categorical variables (e.g., cancer type and treatment response categories). Use contingency tables and chi-squared tests instead.\n\n3. **Time series data**: Measurements taken over time often violate the independence assumption because consecutive observations are correlated.\n\n4. **Restricted range**: If you only observe a narrow range of values, the correlation may not reflect the true relationship across the full range.\n\n5. **Outliers present**: Single extreme values can dramatically inflate or deflate the correlation coefficient, making it unrepresentative of the overall pattern.\n\n**Always plot your data first** before calculating correlation coefficients. Visual inspection will reveal patterns that a single number cannot capture.\n:::\n\n### Clinical Example: Inappropriate Use of Correlation\n\nConsider trying to correlate cancer stage (I, II, III, IV) with treatment type (surgery, chemotherapy, radiotherapy). This is inappropriate because:\n\n- Stage is ordinal categorical (not truly continuous)\n- Treatment type is nominal categorical\n- The relationship is not linear\n- Better approaches: contingency tables, chi-squared test, or logistic regression\n\n## Linear Regression\n\nLinear regression is used when we believe a variable **y** is linearly dependent on another variable **x**. This means that a change in x will lead to a change in y. We use linear regression to determine the linear line (the regression of y on x) that best describes the straight-line relationship between the two variables.\n\n### The Regression Equation\n\nThe equation which estimates the simple linear regression line is:\n\n$$Y = a + bx$$\n\nWhere:\n\n- **x** is usually called the independent, predictor, or explanatory variable\n- **Y** is the value of y (usually called the dependent, outcome, or response variable) which lies on the estimated line. It is an estimate of the value we expect for y if the value of x is known. Y is called the **fitted value** of y\n- **a** and **b** are called the **regression coefficients** of the estimated line\n- **a** is the **intercept** of the estimated line; it is the average value of Y when x = 0\n- **b** is the **slope** of the estimated line; it represents the average amount by which Y increases if we increase x by one unit\n\n### Residuals and Least Squares\n\nThe **residual** is the difference between the actual response y and the predicted response Y from the regression line. The method of **least squares** regression works by minimising the sum of squared residuals.\n\nThe intercept and slope are determined by the **method of least squares** (often called ordinary least squares, OLS). This method determines the line of best fit so that the sum of the squared residuals is at a minimum.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(789)\nn <- 20\n\nreg_data <- tibble(\n  age = runif(n, 20, 75),\n  hb = 8.24 + 0.13 * age + rnorm(n, 0, 1.5)\n)\n\nmodel <- lm(hb ~ age, data = reg_data)\n\n# Add one point to highlight residual\nhighlight_point <- tibble(age = 31, hb = 10.5)\npredicted_hb <- predict(model, newdata = highlight_point)\n\nggplot(reg_data, aes(x = age, y = hb)) +\n  geom_point(colour = \"#2c3e50\", size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"#3498db\", linewidth = 1) +\n  geom_point(data = highlight_point, colour = \"#e74c3c\", size = 3) +\n  geom_segment(aes(x = 31, xend = 31, y = 10.5, yend = predicted_hb),\n               colour = \"#e74c3c\", linetype = \"dashed\") +\n  annotate(\"text\", x = 60, y = 10, \n           label = paste0(\"Hb = \", round(coef(model)[1], 2), \" + \", \n                         round(coef(model)[2], 2), \" × Age\"),\n           size = 4, colour = \"#3498db\") +\n  annotate(\"text\", x = 38, y = 10.2, label = \"residual\", size = 3, colour = \"#e74c3c\") +\n  labs(x = \"Age (years)\", y = \"Haemoglobin (g/dL)\") +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![Scatterplot with fitted regression line showing residuals](09-correlation-regression_files/figure-html/fig-regression-line-1.png){#fig-regression-line width=768}\n:::\n:::\n\n\n### Coefficients, Confidence Intervals, P-values, and R-squared\n\nAs mentioned previously:\n\n- The **intercept (a)** is the average value of the response when the predictor is 0\n- The **slope (b)** is the average change in the response when the predictor increases by 1 unit\n- If the predictor was a binary variable (for example indicating men and women) the slope (b) would indicate the average difference in response between the two groups\n\nThe intercept (a) and the slope (b) are sample estimates of corresponding population parameters. These estimates have an inherent variability which is used to provide **95% confidence intervals** for where the true population parameters may lie.\n\nThe interval for the slope indicates the range, for the wider population, that the change in the response is likely to lie between as the predictor increases by 1 unit. **If this interval includes 0, the coefficient is not statistically different from 0.**\n\nEach coefficient has a **p-value**. The p-value relates to a test of the null hypothesis that the coefficient equals 0, versus the alternative hypothesis that the coefficient does not equal 0:\n\n- If p<0.05 the null hypothesis is rejected in favour of the alternative hypothesis\n- If p>0.05 the null hypothesis cannot be discounted\n\n### R-squared\n\nWe can assess how well the line fits the data by calculating the **coefficient of determination R-squared** (usually expressed as a percentage ranging from 0-100%), which is equal to the square of the correlation coefficient.\n\nThis represents the **percentage of the variability of the response that can be explained by the predictor**. The higher the R-squared, the better the model.\n\n## Assumptions of Linear Regression\n\nMany of the assumptions which underlie regression analysis relate to the distribution of the residuals. The assumptions are:\n\n1. The relationship between the response and predictor is approximately **linear**\n2. The observations in the sample are **independent**\n3. The distribution of residuals is **Normal**\n4. The residuals have **constant variance** (homoscedasticity)\n\n### Checking Assumptions\n\nAssumptions can be checked by examining plots of the residuals. The most common method is to plot the residuals against the fitted values. This plot can show systematic deviations from a linear relationship and highlight non-constant variance. A Normal probability plot or histogram can be used to assess the Normality assumption of residuals.\n\n- A **linear relationship** means that across the range of fitted values, the residuals are spread equally above and below 0\n- **Constant variance** of the residuals means that in a plot of residuals against fitted values, the spread of the residuals doesn't change\n\n### Diagnostic Plots for Checking Assumptions\n\nHere are examples of diagnostic plots showing both good and problematic patterns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(654)\nn <- 100\n\n# Good model - assumptions met\ngood_data <- tibble(\n  x = runif(n, 10, 50),\n  y = 5 + 2*x + rnorm(n, 0, 5)\n)\ngood_model <- lm(y ~ x, data = good_data)\ngood_data$residuals <- residuals(good_model)\ngood_data$fitted <- fitted(good_model)\n\n# Problematic model - non-constant variance (heteroscedasticity)\nbad_data <- tibble(\n  x = runif(n, 10, 50),\n  y = 5 + 2*x + rnorm(n, 0, sd = 0.2*x)  # variance increases with x\n)\nbad_model <- lm(y ~ x, data = bad_data)\nbad_data$residuals <- residuals(bad_model)\nbad_data$fitted <- fitted(bad_model)\n\n# Create diagnostic plots\np1 <- ggplot(good_data, aes(x = fitted, y = residuals)) +\n  geom_point(colour = \"#2c3e50\", alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = \"#e74c3c\") +\n  geom_smooth(se = FALSE, colour = \"#3498db\", linewidth = 0.8) +\n  labs(title = \"(a) Good: residuals randomly scattered\",\n       x = \"Fitted values\", y = \"Residuals\") +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(size = 10))\n\np2 <- ggplot(bad_data, aes(x = fitted, y = residuals)) +\n  geom_point(colour = \"#2c3e50\", alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = \"#e74c3c\") +\n  geom_smooth(se = FALSE, colour = \"#3498db\", linewidth = 0.8) +\n  labs(title = \"(b) Problem: increasing spread (non-constant variance)\",\n       x = \"Fitted values\", y = \"Residuals\") +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(size = 10))\n\np3 <- ggplot(good_data, aes(sample = residuals)) +\n  stat_qq(colour = \"#2c3e50\", alpha = 0.6) +\n  stat_qq_line(colour = \"#3498db\", linewidth = 1) +\n  labs(title = \"(c) Good: points follow diagonal line\",\n       x = \"Theoretical quantiles\", y = \"Sample quantiles\") +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(size = 10))\n\np4 <- ggplot(good_data, aes(x = residuals)) +\n  geom_histogram(bins = 20, fill = \"#3498db\", colour = \"white\", alpha = 0.7) +\n  labs(title = \"(d) Histogram of residuals (should be approximately Normal)\",\n       x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(size = 10))\n\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![Diagnostic plots for checking regression assumptions](09-correlation-regression_files/figure-html/fig-diagnostic-plots-1.png){#fig-diagnostic-plots width=960}\n:::\n:::\n\n\n::: {.callout-tip}\n## Interpreting Diagnostic Plots\n- **Residuals vs Fitted (top left)**: Look for random scatter around zero with no pattern. A funnel shape indicates non-constant variance.\n- **Residuals vs Fitted (top right)**: Shows problematic pattern where spread increases with fitted values.\n- **Q-Q plot (bottom left)**: Points should follow the diagonal line closely. Deviations suggest non-Normal residuals.\n- **Histogram (bottom right)**: Should show approximately bell-shaped, symmetric distribution.\n:::\n\n## Worked Example: CTV and PTV in Prostate Cancer\n\nThe clinical target volume (cm³) (CTV) and planning target volume (PTV) were recorded for 29 patients receiving stereotactic radiotherapy for prostate cancer. The question of interest was what the relationship between PTV and CTV was, and could the average PTV be predicted when the CTV is known.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(321)\nn <- 29\n\nprostate_data <- tibble(\n  ctv = runif(n, 15, 75),\n  ptv = 16.96 + 1.58 * ctv + rnorm(n, 0, 8)\n)\n\nmodel_prostate <- lm(ptv ~ ctv, data = prostate_data)\n\nggplot(prostate_data, aes(x = ctv, y = ptv)) +\n  geom_point(colour = \"#2c3e50\", size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, colour = \"#3498db\", fill = \"#3498db\", alpha = 0.2) +\n  labs(x = \"CTV (cm³)\", y = \"PTV (cm³)\") +\n  theme_minimal(base_size = 14)\n```\n\n::: {.cell-output-display}\n![Relationship between CTV and PTV in prostate cancer patients](09-correlation-regression_files/figure-html/fig-ctv-ptv-1.png){#fig-ctv-ptv width=768}\n:::\n:::\n\n\n\n::: {#tbl-regression-output .cell tbl-cap='Linear regression output for PTV vs CTV'}\n\n```{.r .cell-code}\ntidy(model_prostate, conf.int = TRUE) |>\n  mutate(\n    term = c(\"Intercept\", \"CTV\"),\n    across(where(is.numeric), ~round(., 3))\n  ) |>\n  select(Term = term, Estimate = estimate, `Std. Error` = std.error,\n         `95% CI Lower` = conf.low, `95% CI Upper` = conf.high, \n         `P-value` = p.value) |>\n  kable() |>\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> Std. Error </th>\n   <th style=\"text-align:right;\"> 95% CI Lower </th>\n   <th style=\"text-align:right;\"> 95% CI Upper </th>\n   <th style=\"text-align:right;\"> P-value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Intercept </td>\n   <td style=\"text-align:right;\"> 18.043 </td>\n   <td style=\"text-align:right;\"> 5.430 </td>\n   <td style=\"text-align:right;\"> 6.901 </td>\n   <td style=\"text-align:right;\"> 29.186 </td>\n   <td style=\"text-align:right;\"> 0.003 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CTV </td>\n   <td style=\"text-align:right;\"> 1.550 </td>\n   <td style=\"text-align:right;\"> 0.109 </td>\n   <td style=\"text-align:right;\"> 1.326 </td>\n   <td style=\"text-align:right;\"> 1.773 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe equation of the line is:\n\n$$\\text{PTV} = 16.96 + 1.58 \\times \\text{CTV}$$\n\nThis means that as the CTV increases by 1 cm³, the PTV increases by 1.58 cm³.\n\nThe 95% confidence interval for the slope is (1.32, 1.84). In a wider population of similar prostate cancer patients, as the CTV increases by 1 cm³, the PTV is likely to increase by between 1.32 and 1.84 cm³.\n\nThe R-squared for the model was 0.85. This means that **85% of the variation in PTV was explained by CTV alone**. 15% remains unexplained.\n\n## Multiple Linear Regression\n\n**Multiple linear regression** is an extension of simple linear regression. We would use multiple linear regression when we want to predict a response using several predictor variables. For example, we may wish to predict respiratory muscle strength from weight, age, height and sex.\n\nThe assumptions of multiple linear regression are the same as those for simple linear regression.\n\n### Interpretation of Coefficients in Multiple Regression\n\n- The **intercept (a)** is the average value of the response when all the predictors have values equal to zero\n- If a predictor is **continuous**, its slope (b) indicates the average change in the response when all the other predictors are held constant\n- If a predictor is **binary** (for example indicating men and women), the slope (b) represents the average difference in the response between the groups when all other predictors are held constant\n\nWhen multivariable regression is used, a variation of the R-squared called the **adjusted R-squared** is employed to assess the fit of the model. The adjusted R-squared takes account of the number of predictors used in the model, but its interpretation is the same as for R-squared.\n\n## Summary\n\n| Concept | Description |\n|---------|-------------|\n| Correlation coefficient (r) | Measures strength of linear association (-1 to +1) |\n| Pearson's correlation | For Normally distributed data |\n| Spearman's correlation | Non-parametric alternative |\n| Linear regression | Predicts response from predictor variable |\n| Intercept (a) | Average response when predictor = 0 |\n| Slope (b) | Average change in response per unit increase in predictor |\n| R-squared | Proportion of variance explained by the model |\n| Residuals | Difference between observed and predicted values |\n",
    "supporting": [
      "09-correlation-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}